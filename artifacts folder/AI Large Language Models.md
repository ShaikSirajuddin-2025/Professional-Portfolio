# Training Generative AI Large Language Models (LLMs)

## 📊 Visual Artifact: How Generative AI LLMs Are Trained

This repository showcases a detailed infographic that illustrates the key processes, resources, and examples involved in training large language models (LLMs) such as GPT-4, Claude, and LLaMA.

---

### 🧠 Key Phases in LLM Training:

1. **Data Collection & Preprocessing**
2. **Model Architecture Design**
3. **Training (Compute-Intensive Phase)**
4. **Model Evaluation & Fine-Tuning**
5. **Deployment**

Each phase is visually represented and explained in the infographic below.

---

### 🖼️ Infographic

<p align="center">
  <img src="TRAINING GENERATIVE AI LARGE LANGUAGE MODELS (LLMs).jpg" alt="Visual Map of ML Algorithms" width="1000"/>
</p>

---

### 🔍 Key Resources Required

| Resource        | Role in Training                       |
|----------------|----------------------------------------|
| 📚 Data         | Determines the breadth of model knowledge |
| 🖥️ Compute Power| Affects training time and model complexity |
| ⚡ Energy       | High energy consumption raises environmental concerns |
| ⏱️ Time         | Training may take weeks or months |
| 💰 Cost         | Ranges from millions to tens of millions of dollars |

---

### 💡 Purpose

This artifact is designed to visually communicate the complexity and infrastructure required in generative AI development. It supports my AI/ML competency portfolio by showcasing:
- Technical understanding of LLM training pipelines
- Ability to create compelling visual summaries
- Awareness of real-world cost and sustainability issues in AI

---

### 📂 Portfolio Integration

This artifact is part of my professional portfolio, highlighting my skills in data visualization, AI literacy, and model lifecycle understanding.

> 🔗 [Visit my full portfolio here](#) *(Replace with your actual portfolio link)*

---

